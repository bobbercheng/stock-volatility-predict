{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPU accelerated solution using NVIDIA RAPIDS cudf and cuml\n# Data loading, preprocessing and feature engineering takes less than 3min in GPU.","metadata":{}},{"cell_type":"code","source":"import pandas\nimport numpy as np\nimport cudf as pd\nimport cupy as cp\n\nimport glob\nimport os\nimport gc\nimport time\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.optimize import minimize\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom catboost import Pool, CatBoostRegressor\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport cuml\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import LinearRegression\nfrom cuml import Ridge\nfrom cuml.ensemble import RandomForestRegressor\n\n\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}\n\ndef convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:37.343798Z","iopub.execute_input":"2021-09-22T22:03:37.34409Z","iopub.status.idle":"2021-09-22T22:03:42.770114Z","shell.execute_reply.started":"2021-09-22T22:03:37.344016Z","shell.execute_reply":"2021-09-22T22:03:42.76941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading train and test sets","metadata":{}},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\ntrain = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntest = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n\ntrain['is_train'] = 1\ntest['is_train'] = 0\n\ntrain = convert_to_32bit(train)\ntest = convert_to_32bit(test)\n\nprint( train.shape )\nprint( test.shape )","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:42.771955Z","iopub.execute_input":"2021-09-22T22:03:42.772217Z","iopub.status.idle":"2021-09-22T22:03:46.921261Z","shell.execute_reply.started":"2021-09-22T22:03:42.772184Z","shell.execute_reply":"2021-09-22T22:03:46.920492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.head(20))","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:46.922728Z","iopub.execute_input":"2021-09-22T22:03:46.923002Z","iopub.status.idle":"2021-09-22T22:03:46.950505Z","shell.execute_reply.started":"2021-09-22T22:03:46.922968Z","shell.execute_reply":"2021-09-22T22:03:46.949681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test.head(20))","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:46.951904Z","iopub.execute_input":"2021-09-22T22:03:46.952163Z","iopub.status.idle":"2021-09-22T22:03:46.96544Z","shell.execute_reply.started":"2021-09-22T22:03:46.952129Z","shell.execute_reply":"2021-09-22T22:03:46.964289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking how many stock_id there are in train and test","metadata":{}},{"cell_type":"code","source":"train_stock_ids = train['stock_id'].to_pandas().unique()\ntest_stock_ids = test['stock_id'].to_pandas().unique()\nprint( 'Sizes:', len(train_stock_ids), len(test_stock_ids) )\nprint( 'Train stocks:', train_stock_ids )\nprint( 'Test stocks:', test_stock_ids )","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:46.968987Z","iopub.execute_input":"2021-09-22T22:03:46.969255Z","iopub.status.idle":"2021-09-22T22:03:46.986201Z","shell.execute_reply.started":"2021-09-22T22:03:46.969221Z","shell.execute_reply":"2021-09-22T22:03:46.985373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform(df, groupby='time_id', feat='price', agg='mean' ):\n    return df.merge( \n        df.groupby(groupby)[feat].agg(agg).reset_index().rename({feat:feat+'_'+agg}, axis=1),\n        on=groupby,\n        how='left' \n    )\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    df = df.sort_values(['time_id','seconds_in_bucket']).reset_index(drop=True)\n    \n    # Calculate Wap\n    df['wap1'] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    df['wap2'] = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    df['wap3'] = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    df['wap4'] = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    \n    \n    # Calculate log returns\n    df['log_return1'] = df['wap1'].log()\n    df['log_return1'] = df['log_return1'] - df.groupby(['time_id'])['log_return1'].shift(1).reset_index(drop=True)\n\n    df['log_return2'] = df['wap2'].log()\n    df['log_return2'] = df['log_return2'] - df.groupby(['time_id'])['log_return2'].shift(1).reset_index(drop=True)\n\n    df['log_return3'] = df['wap3'].log()\n    df['log_return3'] = df['log_return3'] - df.groupby(['time_id'])['log_return3'].shift(1).reset_index(drop=True)\n\n    df['log_return4'] = df['wap4'].log()\n    df['log_return4'] = df['log_return4'] - df.groupby(['time_id'])['log_return4'].shift(1).reset_index(drop=True)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    df['log_return1_sqr'] = df['log_return1'] ** 2\n    df['log_return2_sqr'] = df['log_return2'] ** 2\n    df['log_return3_sqr'] = df['log_return3'] ** 2\n    df['log_return4_sqr'] = df['log_return4'] ** 2\n    \n    df = transform(df, groupby='time_id', feat='wap1', agg='median' )\n    df = transform(df, groupby='time_id', feat='wap2', agg='median' )\n    df = transform(df, groupby='time_id', feat='wap3', agg='median' )\n    df = transform(df, groupby='time_id', feat='wap4', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return1', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return2', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return3', agg='median' )\n    df = transform(df, groupby='time_id', feat='log_return4', agg='median' )\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': ['sum', 'std', 'min','max'],\n        'wap2': ['sum', 'std', 'min','max'],\n        'wap3': ['sum', 'std', 'min','max'],\n        'wap4': ['sum', 'std', 'min','max'],\n        'log_return1_sqr': ['sum', 'std', 'min','max'],\n        'log_return2_sqr': ['sum', 'std', 'min','max'],\n        'log_return3_sqr': ['sum', 'std', 'min','max'],\n        'log_return4_sqr': ['sum', 'std', 'min','max'],\n        'wap_balance': ['sum', 'mean', 'min','max'],\n        'price_spread':['sum', 'mean', 'min','max'],\n        'price_spread2':['sum', 'mean', 'min','max'],\n        'bid_spread':['sum', 'mean', 'min','max'],\n        'ask_spread':['sum', 'mean', 'min','max'],\n        'total_volume':['sum', 'mean', 'min','max'],\n        'volume_imbalance':['sum', 'mean', 'min','max'],\n        \"bid_ask_spread\":['sum',  'mean', 'min','max'],\n        \n        \"wap1_median\":['sum',  'mean', 'min','max'],\n        \"wap2_median\":['sum',  'mean', 'min','max'],\n        \"wap3_median\":['sum',  'mean', 'min','max'],\n        \"wap4_median\":['sum',  'mean', 'min','max'],\n        \"log_return1_median\":['mean',  'std', 'min','max'],\n        \"log_return2_median\":['mean',  'std', 'min','max'],\n        \"log_return3_median\":['mean',  'std', 'min','max'],\n        \"log_return4_median\":['mean',  'std', 'min','max'],\n    }\n    create_feature_dict_time = {\n        'log_return1_sqr': ['sum', 'std', 'min','max'],\n        'log_return2_sqr': ['sum', 'std', 'min','max'],\n        'log_return3_sqr': ['sum', 'std', 'min','max'],\n        'log_return4_sqr': ['sum', 'std', 'min','max'],\n    }\n    \n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:46.98762Z","iopub.execute_input":"2021-09-22T22:03:46.987891Z","iopub.status.idle":"2021-09-22T22:03:47.024711Z","shell.execute_reply.started":"2021-09-22T22:03:46.987858Z","shell.execute_reply":"2021-09-22T22:03:47.023826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    df = df.sort_values('time_id').reset_index(drop=True)\n\n    df['log_return'] = df['price'].log()\n    df['log_return'] = df['log_return'] - df.groupby(['time_id'])['log_return'].shift(1).reset_index(drop=True)\n    df['log_return_sqr'] = df['log_return'] ** 2\n    \n    df['amount']=df['price']*df['size']\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return_sqr': ['sum', 'std','max', 'min'],\n        'seconds_in_bucket':['nunique','std', 'mean','max', 'min'],\n        'size':['sum', 'nunique','std','max', 'min'],\n        'order_count':['sum','nunique','max','min','std'],\n        'amount':['sum','std','max','min'],\n    }\n    create_feature_dict_time = {\n        'log_return_sqr': ['sum', 'std','max','min'],\n        'seconds_in_bucket':['nunique'],\n        'size':['sum','mean','std','min','max'],\n        'order_count':['sum','mean','std','min','max'],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_250 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 250, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    df = df.sort_values(['time_id','seconds_in_bucket']).reset_index(drop=True)\n    \n    df = transform(df, groupby='time_id', feat='price', agg='mean' )\n    df = transform(df, groupby='time_id', feat='price', agg='sum' )\n    df = transform(df, groupby='time_id', feat='size', agg='mean' )\n    df['price_dif'] = ((df['price'] - df.groupby(['time_id'])['price'].shift(1).reset_index(drop=True)) / df['price']).fillna(0.)\n    df['tendencyV'] = df['size'] * df['price_dif']\n    df['f_max'] = 1 * (df['price'] >= df['price_mean'])\n    df['f_min'] = 1 * (df['price'] < df['price_mean'])\n    df['df_max'] = 1 * (df['price_dif'] >= 0)\n    df['df_min'] = 1 * (df['price_dif'] < 0)\n    df['abs_dif'] = (df['price'] - df['price_mean']).abs()\n    df['price_sqr'] = df['price']**2\n    df['size_dif'] = (df['size'] - df['size_mean']).abs()\n    df['size_sqr'] = df['size']**2\n    df['iqr_p25'] = df.groupby(['time_id'])['price'].quantile(0.15).reset_index(drop=True)\n    df['iqr_p75'] = df.groupby(['time_id'])['price'].quantile(0.85).reset_index(drop=True)\n    df['iqr_p_v25'] = df.groupby(['time_id'])['size'].quantile(0.15).reset_index(drop=True)\n    df['iqr_p_v75'] = df.groupby(['time_id'])['size'].quantile(0.85).reset_index(drop=True)\n\n    df = transform(df, groupby='time_id', feat='price_dif', agg='std' )\n    df = transform(df, groupby='time_id', feat='tendencyV', agg='std' )\n    df = transform(df, groupby='time_id', feat='f_max', agg='std' )\n    df = transform(df, groupby='time_id', feat='f_min', agg='std' )\n    df = transform(df, groupby='time_id', feat='df_max', agg='std' )\n    df = transform(df, groupby='time_id', feat='df_min', agg='std' )\n    df = transform(df, groupby='time_id', feat='size_dif', agg='std' )\n    \n    dt = df.groupby('time_id')[['tendencyV','price','price_dif','f_max','f_min','df_max','df_min','abs_dif','price_sqr','size_dif','size_sqr','iqr_p25','iqr_p75','iqr_p_v25','iqr_p_v75','price_dif_std','tendencyV_std','f_max_std','f_min_std','df_max_std','df_min_std','size_dif_std']].agg(\n        {\n            'tendencyV':['sum','std','max', 'min'],\n            'price':['mean','std','max', 'min'],\n            'price_dif':['mean','std','max', 'min'],\n            'f_max':['mean','std','max', 'min'],\n            'f_min':['mean','std','max', 'min'],\n            'df_max':['mean','std','max', 'min'],\n            'df_min':['mean','std','max', 'min'],\n            'abs_dif':['median','std','max', 'min'],\n            'price_sqr':['sum','std','max', 'min'],\n            'size_dif':['median','std','max', 'min'],\n            'size_sqr':['sum','std','max', 'min'],\n            'iqr_p25':['mean','std','max', 'min'],\n            'iqr_p75':['mean','std','max', 'min'],\n            'iqr_p_v25':['mean','std','max', 'min'],\n            'iqr_p_v75':['mean','std','max', 'min'],\n            'price_dif_std':['mean','std','max', 'min'],\n            'tendencyV_std':['mean','std','max', 'min'],\n            'f_max_std':['mean','std','max', 'min'],\n            'f_min_std':['mean','std','max', 'min'],\n            'df_max_std':['mean','std','max', 'min'],\n            'df_min_std':['mean','std','max', 'min'],\n            'size_dif_std':['mean','std','max', 'min'],\n        }\n    )\n    dt.columns = [i+'_'+j for i, j in dt.columns] \n    df_feature = df_feature.merge(dt, left_on='time_id_', right_index=True, how='left')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_250, how = 'left', left_on = 'time_id_', right_on = 'time_id__250')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature = df_feature.sort_values(['time_id_' ]).reset_index(drop=True)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__250', 'time_id__200', 'time_id_','time_id__100','stock_id'], axis = 1, inplace = True)\n\n    fnames = ['trade_' + f for f in df_feature.columns]\n    fnames[-1] = 'row_id'\n    df_feature.columns = fnames\n\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:47.026413Z","iopub.execute_input":"2021-09-22T22:03:47.02677Z","iopub.status.idle":"2021-09-22T22:03:47.055677Z","shell.execute_reply.started":"2021-09-22T22:03:47.026732Z","shell.execute_reply":"2021-09-22T22:03:47.054746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process all train .parquet files. Create features using cudf (GPU)\n# Note cudf speed to load and apply all feature engineering in all train set stocks.","metadata":{}},{"cell_type":"code","source":"%%time\nDF_TRAIN = []\nfor stock_id in tqdm(train_stock_ids, disable=True):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'train_parquet/'+str(stock_id)+'.parquet' )\n    DF_TRAIN.append(df_tmp)\n\n# Concatenate all stock_id in the same dataframe\nDF_TRAIN = pd.concat(DF_TRAIN, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train/test rows\nDF_TRAIN['is_test'] = 0\nDF_TRAIN.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:03:47.05744Z","iopub.execute_input":"2021-09-22T22:03:47.057729Z","iopub.status.idle":"2021-09-22T22:07:45.21975Z","shell.execute_reply.started":"2021-09-22T22:03:47.057701Z","shell.execute_reply":"2021-09-22T22:07:45.21905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process all test .parquet files. Create features using cudf (GPU)","metadata":{}},{"cell_type":"code","source":"%%time\nDF_TEST = []\nfor stock_id in tqdm(test_stock_ids, disable=True):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'test_parquet/'+str(stock_id)+'.parquet' )\n    DF_TEST.append(df_tmp)\n    \n# Concatenate all stock_id in the same dataframe\nDF_TEST = pd.concat(DF_TEST, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train/test rows\nDF_TEST['is_test'] = 1\nDF_TEST.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:45.221069Z","iopub.execute_input":"2021-09-22T22:07:45.221491Z","iopub.status.idle":"2021-09-22T22:07:46.253762Z","shell.execute_reply.started":"2021-09-22T22:07:45.221454Z","shell.execute_reply":"2021-09-22T22:07:46.25307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN = pd.concat( [DF_TRAIN, DF_TEST] ).sort_values(['stock_id','time_id_']).reset_index(drop=True)\n\ndel DF_TRAIN, DF_TEST\n_ = gc.collect()\nTRAIN.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:46.254905Z","iopub.execute_input":"2021-09-22T22:07:46.255611Z","iopub.status.idle":"2021-09-22T22:07:46.887187Z","shell.execute_reply.started":"2021-09-22T22:07:46.255575Z","shell.execute_reply":"2021-09-22T22:07:46.886388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef get_time_stock(df_):\n    vol_cols = ['log_return1_sqr_sum_500', 'log_return2_sqr_sum_500', 'log_return3_sqr_sum_500', 'log_return4_sqr_sum_500', 'trade_log_return_sqr_sum', 'trade_log_return_sqr_std', 'trade_seconds_in_bucket_nunique' ]\n\n    df = df_.copy()\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_stock_id.columns = ['_'.join(col) + '_stock' for col in df_stock_id.columns]\n\n    df_time_id = df.groupby(['time_id_'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col)+ '_time' for col in df_time_id.columns]\n    \n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id_'], right_on = ['time_id___time'])\n    df.drop(['stock_id__stock', 'time_id___time'], axis = 1, inplace = True)\n    return df\n\nTRAIN_ = get_time_stock(TRAIN)\nTRAIN_.drop(['stock_id','time_id_'], axis = 1, inplace = True)\nprint(TRAIN_.shape)\nprint(TRAIN_.head())","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:46.888463Z","iopub.execute_input":"2021-09-22T22:07:46.888799Z","iopub.status.idle":"2021-09-22T22:07:47.649988Z","shell.execute_reply.started":"2021-09-22T22:07:46.888756Z","shell.execute_reply":"2021-09-22T22:07:47.649059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(TRAIN_, on='row_id', how='left' )\ntest  = test.merge(TRAIN_, on='row_id', how='left' )\n\ndel TRAIN_, TRAIN\n_ = gc.collect()\n\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:47.651417Z","iopub.execute_input":"2021-09-22T22:07:47.651857Z","iopub.status.idle":"2021-09-22T22:07:48.035276Z","shell.execute_reply.started":"2021-09-22T22:07:47.651818Z","shell.execute_reply":"2021-09-22T22:07:48.034473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:48.036718Z","iopub.execute_input":"2021-09-22T22:07:48.036986Z","iopub.status.idle":"2021-09-22T22:07:48.632996Z","shell.execute_reply.started":"2021-09-22T22:07:48.036953Z","shell.execute_reply":"2021-09-22T22:07:48.632376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now time to calculate correlation between all stock. The best way is using a correlation matrix, so first pivot all target variables by stock_id, then just calculate the correlation matrix.\n# To Find correlated stocks use Kmeans algorithm on the correlation matrix. This procedure is a bit leak because it not being processed using crossvalidation, but it won't leak much since only 6 clusters are being calculated.","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns=['stock_id'], values=['target']).fillna(0.)\ncorr = train_p.corr()\n\nkm = cuml.KMeans(n_clusters=6, max_iter=2000, n_init=5).fit(corr)\ndf = pd.DataFrame( {'stock_id': [ f[1] for f in corr.columns ], 'cluster': km.labels_} )\ndf = convert_to_32bit(df)\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n\ndel train_p, df, corr, km\n_ = gc.collect()\n\n# Clusters found\ntrain.groupby('cluster')['time_id'].agg('count')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:48.636344Z","iopub.execute_input":"2021-09-22T22:07:48.636556Z","iopub.status.idle":"2021-09-22T22:07:52.851363Z","shell.execute_reply.started":"2021-09-22T22:07:48.636533Z","shell.execute_reply":"2021-09-22T22:07:52.850706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matTrain = []\nmatTest = []\n\n# 6 clusters\nfor ind in range(train.cluster.max()+1):\n    print(ind)\n    newDf = train.loc[train['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTrain.append ( newDf )\n    \n    newDf = test.loc[test['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTest.append ( newDf )\n    \nmatTrain = pd.concat(matTrain).reset_index()\nmatTrain.drop(columns=['target'],inplace=True)\n\nmatTest = pd.concat(matTest).reset_index()\n\nmatTrain.shape, matTest.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:52.852423Z","iopub.execute_input":"2021-09-22T22:07:52.852683Z","iopub.status.idle":"2021-09-22T22:07:56.132169Z","shell.execute_reply.started":"2021-09-22T22:07:52.852649Z","shell.execute_reply":"2021-09-22T22:07:56.131405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matTest = pd.concat([matTest , matTrain.loc[matTrain.time_id==5]])\nmatTrain = matTrain.pivot(index='time_id', columns='stock_id')\nmatTrain.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTrain.columns]\nmatTrain.reset_index(inplace=True)\n\nmatTest = matTest.pivot(index='time_id', columns='stock_id')\nmatTest.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTest.columns]\nmatTest.reset_index(inplace=True)\n\nmatTrain.shape, matTest.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:56.13372Z","iopub.execute_input":"2021-09-22T22:07:56.13402Z","iopub.status.idle":"2021-09-22T22:07:57.783021Z","shell.execute_reply.started":"2021-09-22T22:07:56.133979Z","shell.execute_reply":"2021-09-22T22:07:57.782137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfeatures = [\n    'time_id',\n        \n    'wap1_sum_stock127',\n    'wap1_sum_stock128',     \n    'wap1_sum_stock129',\n    'wap1_sum_stock130',     \n    'wap1_sum_stock131',\n    'wap1_sum_stock132',\n        \n    'wap2_sum_stock127',\n    'wap2_sum_stock128',     \n    'wap2_sum_stock129',\n    'wap2_sum_stock130',     \n    'wap2_sum_stock131',\n    'wap2_sum_stock132',\n        \n    'wap3_sum_stock127',\n    'wap3_sum_stock128',     \n    'wap3_sum_stock129',\n    'wap3_sum_stock130',     \n    'wap3_sum_stock131',\n    'wap3_sum_stock132',\n        \n    'wap4_sum_stock127',\n    'wap4_sum_stock128',     \n    'wap4_sum_stock129',\n    'wap4_sum_stock130',     \n    'wap4_sum_stock131',\n    'wap4_sum_stock132',\n    \n    'log_return1_sqr_sum_stock127',\n    'log_return1_sqr_sum_stock128',     \n    'log_return1_sqr_sum_stock129',\n    'log_return1_sqr_sum_stock130',     \n    'log_return1_sqr_sum_stock131',\n    'log_return1_sqr_sum_stock132',\n\n    'log_return2_sqr_sum_stock127',\n    'log_return2_sqr_sum_stock128',     \n    'log_return2_sqr_sum_stock129',\n    'log_return2_sqr_sum_stock130',     \n    'log_return2_sqr_sum_stock131',\n    'log_return2_sqr_sum_stock132',\n\n    'log_return3_sqr_sum_stock127',\n    'log_return3_sqr_sum_stock128',     \n    'log_return3_sqr_sum_stock129',\n    'log_return3_sqr_sum_stock130',     \n    'log_return3_sqr_sum_stock131',\n    'log_return3_sqr_sum_stock132',\n\n    'log_return4_sqr_sum_stock127',\n    'log_return4_sqr_sum_stock128',     \n    'log_return4_sqr_sum_stock129',\n    'log_return4_sqr_sum_stock130',     \n    'log_return4_sqr_sum_stock131',\n    'log_return4_sqr_sum_stock132',\n    \n    'total_volume_sum_stock127',\n    'total_volume_sum_stock128', \n    'total_volume_sum_stock129',\n    'total_volume_sum_stock130', \n    'total_volume_sum_stock131',\n    'total_volume_sum_stock132',\n    \n    'trade_size_sum_stock127',\n    'trade_size_sum_stock128', \n    'trade_size_sum_stock129',\n    'trade_size_sum_stock130', \n    'trade_size_sum_stock131',\n    'trade_size_sum_stock132',\n    \n    'trade_order_count_sum_stock127',\n    'trade_order_count_sum_stock128',\n    'trade_order_count_sum_stock129',\n    'trade_order_count_sum_stock130',\n    'trade_order_count_sum_stock131',      \n    'trade_order_count_sum_stock132',\n    \n    'price_spread_sum_stock127',\n    'price_spread_sum_stock128',\n    'price_spread_sum_stock129',\n    'price_spread_sum_stock130',\n    'price_spread_sum_stock131',   \n    'price_spread_sum_stock132',\n    \n    'bid_spread_sum_stock127',\n    'bid_spread_sum_stock128',\n    'bid_spread_sum_stock129',\n    'bid_spread_sum_stock130',\n    'bid_spread_sum_stock131',\n    'bid_spread_sum_stock132',\n    \n    'ask_spread_sum_stock127',\n    'ask_spread_sum_stock128',\n    'ask_spread_sum_stock129',\n    'ask_spread_sum_stock130',\n    'ask_spread_sum_stock131',   \n    'ask_spread_sum_stock132',\n    \n    'volume_imbalance_sum_stock127',\n    'volume_imbalance_sum_stock128',\n    'volume_imbalance_sum_stock129',\n    'volume_imbalance_sum_stock130',\n    'volume_imbalance_sum_stock131',       \n    'volume_imbalance_sum_stock132',\n    \n    'bid_ask_spread_sum_stock127',\n    'bid_ask_spread_sum_stock128',\n    'bid_ask_spread_sum_stock129',\n    'bid_ask_spread_sum_stock130',\n    'bid_ask_spread_sum_stock131',\n    'bid_ask_spread_sum_stock132',\n]\nmatTrain = convert_to_32bit(matTrain)\nmatTest = convert_to_32bit(matTest)\n\ntrain = pd.merge(train,matTrain[kfeatures],how='left',on='time_id')\ntest = pd.merge(test,matTest[kfeatures],how='left',on='time_id')\n_ = gc.collect()\n\nprint(train.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:07:57.784488Z","iopub.execute_input":"2021-09-22T22:07:57.784751Z","iopub.status.idle":"2021-09-22T22:08:00.238839Z","shell.execute_reply.started":"2021-09-22T22:07:57.784719Z","shell.execute_reply":"2021-09-22T22:08:00.238084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train=train[~(train[\"stock_id\"]==31)].reset_index(drop=True)\n# _= gc.collect()\n\ntrain = convert_to_32bit(train)\ntest  = convert_to_32bit(test)\n_= gc.collect()\n\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:08:00.239893Z","iopub.execute_input":"2021-09-22T22:08:00.240605Z","iopub.status.idle":"2021-09-22T22:08:00.487979Z","shell.execute_reply.started":"2021-09-22T22:08:00.240567Z","shell.execute_reply":"2021-09-22T22:08:00.487178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_target = train.target.to_pandas() #need to be numpy or pandas for sklearn \ntime_id = train.time_id.to_pandas()\nNFOLD = 5\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Target min and max values\nnp.min(y_target), np.max(y_target)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:08:00.489125Z","iopub.execute_input":"2021-09-22T22:08:00.489455Z","iopub.status.idle":"2021-09-22T22:08:00.50118Z","shell.execute_reply.started":"2021-09-22T22:08:00.48942Z","shell.execute_reply":"2021-09-22T22:08:00.500328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost GPU","metadata":{}},{"cell_type":"code","source":"xgbtime = time.time()\n\n# Define the custom metric to optimize\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    err = rmspe(labels, preds)\n    return 'rmspe', err\n\ndef train_and_evaluate_xgb(train, test, params, colNames):\n    # Sample weight\n    train['target_sqr'] = 1. / (train['target'] ** 1.60 + 9e-7)    \n    train.loc[train.stock_id==31,'target_sqr'] = 0.0001\n\n    dtest = xgb.DMatrix(test[colNames])\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        dtrain = xgb.DMatrix(train.loc[train_idx, colNames], train.loc[train_idx, 'target']*10000, weight=train.loc[train_idx, 'target_sqr'])\n        dvalid = xgb.DMatrix(train.loc[valid_idx, colNames], train.loc[valid_idx, 'target']*10000)\n        model = xgb.train(\n            params,\n            dtrain,\n            3000,\n            #[(dtrain, \"train\"), (dvalid, \"valid\")],\n            [(dvalid, \"valid\")],\n            verbose_eval=250,\n            early_stopping_rounds=50,\n            feval=evalerror,\n        )\n        y_train[valid_idx] = np.clip(model.predict(dvalid)/10000, 2e-4, 0.072)\n        y_test += np.clip((model.predict(dtest))/10000, 2e-4, 0.072)\n        print( 'Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )\n    y_test /= NFOLD\n    \n    print( 'XGBoost Rmspe CV:', rmspe(y_target, y_train) )\n    print( pandas.DataFrame.from_dict( model.get_score(), orient='index').sort_values(0, ascending=False).head(20) )\n    print()\n    \n    del model, dtest, dtrain, dvalid\n    _ = gc.collect()\n    \n    return y_train, y_test\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('min')<0 ]\nparams = {\n        \"subsample\": 0.60,\n        \"colsample_bytree\": 0.40,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 0,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.001, \n        'seed' : 2021,\n        'single_precision_histogram': False,\n    }\ny_train1a, y_test1a = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('max')<0 ]\nparams = {\n        \"subsample\": 0.85,\n        \"colsample_bytree\": 0.25,\n        \"max_depth\": 7,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 0,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.001, \n        'seed' : 2022,\n        'single_precision_histogram': False,\n    }\ny_train1b, y_test1b = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ny_train1 = 0.5*y_train1a + 0.5*y_train1b\ny_test1  = 0.5*y_test1a  + 0.5*y_test1b\n_= gc.collect()\nxgbtime = time.time() - xgbtime\n\nprint( 'XGBoost Rmspe CV:', rmspe(y_target, y_train1), 'time: ', int(xgbtime), 's', y_test1[:3] )","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:08:00.502654Z","iopub.execute_input":"2021-09-22T22:08:00.502939Z","iopub.status.idle":"2021-09-22T22:10:46.426844Z","shell.execute_reply.started":"2021-09-22T22:08:00.502902Z","shell.execute_reply":"2021-09-22T22:10:46.425964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catbtime = time.time()\n\ndef train_and_evaluate_catb(train, test, params):\n\n    # Sample weight\n    train['target_sqr'] = 1. / (train['target'] ** 1.75 + 1e-6)\n    train.loc[train.stock_id==31,'target_sqr'] = 1.\n\n    colNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n\n        model = CatBoostRegressor(\n            iterations=3000,\n            learning_rate=0.05,\n            depth=7,\n            loss_function='RMSE',\n            #l2_leaf_reg = 0.001,\n            #random_strength = 0.5,\n            #bagging_temperature = 1.0,\n            task_type=\"GPU\",\n            random_seed = 2021,\n        )        \n        model.fit(\n            X=train.loc[train_idx, colNames].to_pandas(), y=train.loc[train_idx, 'target'].to_pandas(),\n            sample_weight = train.loc[train_idx, 'target_sqr'].to_pandas(),\n            eval_set = (train.loc[valid_idx, colNames].to_pandas(), train.loc[valid_idx, 'target'].to_pandas(),),\n            early_stopping_rounds = 20,\n            cat_features = [0],\n            verbose=False)\n\n        y_train[valid_idx] = np.clip(model.predict(train.loc[valid_idx, colNames].to_pandas()), 2e-4, 0.072)\n        y_test += np.clip((model.predict(test[colNames].to_pandas())), 2e-4, 0.072)\n        print( 'Catboost Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )        \n        print()\n    y_test /= NFOLD\n    return y_train, y_test\n\n\ny_train2, y_test2 = train_and_evaluate_catb(train, test, params)\n_= gc.collect()\ncatbtime = time.time() - catbtime\n     \nprint( 'Catboost Rmspe CV:', rmspe(y_target, y_train2), 'time: ', int(catbtime), 's', y_test2[:3]  )","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:10:46.42817Z","iopub.execute_input":"2021-09-22T22:10:46.428644Z","iopub.status.idle":"2021-09-22T22:17:27.964032Z","shell.execute_reply.started":"2021-09-22T22:10:46.428603Z","shell.execute_reply":"2021-09-22T22:17:27.963128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM GPU","metadata":{}},{"cell_type":"code","source":"lgbtime = time.time()\n\n# Define the custom metric to optimize\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n    y = train['target']\n    \n    y_train = np.zeros(train.shape[0])\n    y_test = np.zeros(test.shape[0])\n    \n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (trn_ind, val_ind) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_tra, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        \n        train_dataset = lgb.Dataset(x_train[features], y_tra, weight = (1. / (np.square(y_tra) + 1e-6)) )\n        valid_dataset = lgb.Dataset(x_val[features], y_val)\n        model = lgb.train(params = params,\n                          num_boost_round=3000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, valid_dataset], \n                          verbose_eval = 100,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        y_train[val_ind] = np.clip(model.predict(x_val[features]), 2e-4, 0.072)\n        y_test += np.clip((model.predict(test[features])), 2e-4, 0.072)        \n    y_test/=NFOLD\n    \n    print('LightGBM Rmspe Fold:', rmspe(y_target, y_train))\n    lgb.plot_importance(model,max_num_features=20)\n    \n    return y_train, y_test\n\n\nparams = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':255,\n    'min_data_in_leaf':750,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':2021,\n    'n_jobs':-1,\n    'verbose': -1,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id':-1,\n    'gpu_device_id':-1,\n    'gpu_use_dp': False,\n}\n\ntrain_pandas = train.to_pandas()\ntest_pandas = test.to_pandas()\ndel train,test\n\n\ny_train3, y_test3 = train_and_evaluate_lgb(train_pandas, test_pandas, params)\n_= gc.collect()\n\nprint( 'LightGBM Rmspe CV:', rmspe(y_target, y_train3), 'time: ', int(time.time() - lgbtime), 's', y_test3[:3]   )","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:17:27.965666Z","iopub.execute_input":"2021-09-22T22:17:27.96618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neuron Network","metadata":{}},{"cell_type":"markdown","source":"## Baseline","metadata":{}},{"cell_type":"code","source":"#reset Keras Session\ndef reset_keras():\n    sess = tf.compat.v1.keras.backend.get_session()\n    tf.compat.v1.keras.backend.clear_session()\n    sess.close()\n    sess = tf.compat.v1.keras.backend.get_session()\n\n    # use the same config as you used to create the session\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 1\n    config.gpu_options.visible_device_list = \"0\"\n    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom numpy.random import seed\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as nptensorflow\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\nnn_base_time = time.time()\n\ndef root_mean_squared_per_error(y_true, y_pred):\n#     tf.print('y_true', y_true)\n#     tf.print('y_pred', y_pred)\n    return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n\ndef base_model(max_id_num, feature_num, stock_embedding_size, hidden_units, dropout_rates):\n    print('base_model', max_id_num, feature_num, stock_embedding_size, hidden_units, dropout_rates)\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(feature_num,), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max_id_num+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n#     # Add one or more hidden layers\n    for i in range(len(hidden_units)):\n        out = keras.layers.Dense(hidden_units[i], activation='swish')(out)\n#         out = keras.layers.Dropout(dropout_rates[i])(out)\n        \n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model\n\ndef train_and_evaluate_nn_base(train, test, params):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n    y = train['target']\n    \n    y_train = np.zeros(train.shape[0])\n    y_test = np.zeros(test.shape[0])\n    \n    print('Check null in train', train[features].isnull().sum())\n    print('Check null in test', test[features].isnull().sum())\n    train[features] = train[features].fillna(train[features].mean())\n    test[features] = test[features].fillna(test[features].mean())\n    \n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (trn_ind, val_ind) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_tra, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        \n        cat_data = x_train['stock_id']\n        features.remove('stock_id')\n\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        num_data = x_train[features]\n        num_data = scaler.fit_transform(num_data.values)\n        cat_data_val = x_val['stock_id']\n        num_data_val = x_val[features]\n        num_data_val = scaler.transform(num_data_val.values)\n        \n        model = base_model(max(cat_data), len(features), params['stock_embedding_size'], params['hidden_units'], params['dropout_rates'])\n\n        model.compile(\n#             keras.optimizers.Adam(learning_rate=0.006),\n            keras.optimizers.Adam(learning_rate=params['learning_rate']),\n            loss=root_mean_squared_per_error\n        )\n        \n#         \n\n        es = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss', patience=20, verbose=0,\n            mode='min',restore_best_weights=True)\n\n        plateau = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', factor=0.2, patience=7, verbose=0,\n            mode='min')\n\n#         print(cat_data, num_data)\n#         print(cat_data.shape, num_data.shape)\n#         print(y_tra, y_tra.shape)\n#         print(cat_data_val, num_data_val)\n#         print(cat_data_val.shape, num_data_val.shape)\n#         print(y_val, y_val.shape)\n\n        model.fit([cat_data, num_data], \n                  y_tra,               \n#                   batch_size=2048,\n#                   epochs=1000,\n                  batch_size=params['batch_size'],\n                  epochs=params['epochs'],\n                  validation_data=([cat_data_val, num_data_val], y_val),\n                  callbacks=[es, \n#                              plateau\n                            ],\n                  validation_batch_size=len(y_val),\n                  shuffle=True,\n                 verbose = 1)\n\n        y_train[val_ind] = np.clip(model.predict([cat_data_val, num_data_val]).reshape(1,-1)[0], 2e-4, 0.072)\n        test_nn =scaler.transform(test[features].values)\n        y_test += np.clip(model.predict([test['stock_id'], test_nn]).reshape(1,-1)[0], 2e-4, 0.072)\n        features.append('stock_id')\n        \n        print(y_train[val_ind][:3], y_test[:3])\n        \n        #Delete model and release GPU memory\n        del model, cat_data, num_data, cat_data_val, num_data_val, scaler\n        reset_keras()\n        gc.collect()\n    y_test/=NFOLD\n    \n    print('NN base Rmspe Fold:', rmspe(y_target, y_train))\n    \n    return y_train, y_test\n\nparams = {\n    'batch_size': 2048,\n    'epochs': 1000,\n    'learning_rate': 0.006,\n    'stock_embedding_size': 24,\n    'hidden_units': [128,64,32],\n    'dropout_rates': [0.8, 0.9, 0.9]\n}\n\ny_nn_train1, y_nn_test1 = train_and_evaluate_nn_base(train_pandas, test_pandas, params)\n_= gc.collect()\n\nprint( 'NN base Rmspe CV:', rmspe(y_target, y_nn_train1), 'time: ', int(time.time() - nn_base_time), 's', y_nn_test1[:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling Time","metadata":{}},{"cell_type":"code","source":"print( 'LightGBM Rmspe:', rmspe(y_target, y_train3) )\nprint( 'XGBoost Rmspe:', rmspe(y_target, y_train1) )\nprint( 'CatBoost Rmspe:', rmspe(y_target, y_train2) )\nprint( 'NN base Rmspe:', rmspe(y_target, y_nn_train1) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENSEMBLING_SIZE = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def minimize_arit(W):\n    ypred = W[0] * y_train1 + W[1] * y_train2 + W[2] * y_train3 + W[3] * y_nn_train1\n    return rmspe(y_target, ypred )\n\nW0 = minimize(minimize_arit, [1./ENSEMBLING_SIZE]*ENSEMBLING_SIZE, options={'gtol': 1e-6, 'disp': True}).x\nprint('Weights arit:',W0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def signed_power(var, p=2):\n    return np.sign(var) * np.abs(var)**p\n\ndef minimize_geom(W):\n    ypred = signed_power(y_train1, W[0]) * signed_power(y_train2, W[1]) * signed_power(y_train3, W[2]) * signed_power(y_nn_train1, W[3])\n    return rmspe(y_target, ypred)\n\nW1 = minimize(minimize_geom, [1./ENSEMBLING_SIZE]*ENSEMBLING_SIZE, options={'gtol': 1e-6, 'disp': True}).x\n\nprint('weights geom:',W1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred0 = W0[0] * y_train1 + W0[1] * y_train2 + W0[2] * y_train3 + W0[3] * y_nn_train1\nprint( np.min(ypred0), np.max(ypred0))\n\nypred1 = signed_power(y_train1, W1[0]) * signed_power(y_train2, W1[1]) * signed_power(y_train3, W1[2]) * signed_power(y_nn_train1, W1[3])\nprint( np.min(ypred1) , np.max(ypred1) )\n\nprint( 'Ensemble:', rmspe(y_target, np.clip((ypred0+ypred1)/2 ,0.0002, 0.071) ) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( np.min(ypred0),np.mean(ypred0),np.max(ypred0),np.std(ypred0) )\nprint( np.min(ypred1),np.mean(ypred1),np.max(ypred1),np.std(ypred1) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(ypred0, bins=100)\nplt.hist(ypred1, bins=100, alpha=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pandas['ypred'] = np.clip((ypred0+ypred1)/2,0.0002, 0.071)\ntrain_pandas['error'] = (train_pandas['target'] - train_pandas['ypred']) / train_pandas['target']\ntrain_pandas['error'] = train_pandas['error']**2\n\ndt = train_pandas.groupby('stock_id')['error'].agg('mean').reset_index()\ndt['error'] = np.sqrt(dt['error'])\ndt = dt.sort_values('error', ascending=False)\ndt.to_csv('error-contribution.csv', index=False)\ndel train_pandas['ypred'], train_pandas['error']\ndt.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:35:42.808222Z","iopub.execute_input":"2021-09-22T22:35:42.808817Z","iopub.status.idle":"2021-09-22T22:35:42.854540Z","shell.execute_reply.started":"2021-09-22T22:35:42.808781Z","shell.execute_reply":"2021-09-22T22:35:42.853547Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"dt.tail(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:34:31.283510Z","iopub.execute_input":"2021-09-22T22:34:31.284093Z","iopub.status.idle":"2021-09-22T22:34:31.294185Z","shell.execute_reply.started":"2021-09-22T22:34:31.284053Z","shell.execute_reply":"2021-09-22T22:34:31.293165Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"ypred0 = W0[0] * y_test1 + W0[1] * y_test2 + W0[2] * y_test3 + W0[3] * y_nn_test1\nypred1 = signed_power(y_test1, W1[0]) * signed_power(y_test2, W1[1]) * signed_power(y_test3, W1[2]) * signed_power(y_nn_test1, W1[3])\n\nypredtest = np.clip((ypred0+ypred1)/2,0.0002, 0.071)\nprint( ypred0[:3],  ypred1[:3], ypredtest[:3] )\n\ntest_pandas['target'] = ypredtest\ntest_pandas[['row_id', 'target']].to_csv('submission.csv',index = False)\ntest_pandas[['row_id', 'target']].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T22:34:54.887267Z","iopub.execute_input":"2021-09-22T22:34:54.887848Z","iopub.status.idle":"2021-09-22T22:34:54.912772Z","shell.execute_reply.started":"2021-09-22T22:34:54.887812Z","shell.execute_reply":"2021-09-22T22:34:54.911986Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}